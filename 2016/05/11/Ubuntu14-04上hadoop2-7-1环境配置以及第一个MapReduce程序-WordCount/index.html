
 <!DOCTYPE HTML>
<html lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
  
    <title>Ubuntu14.04上hadoop2.7.1环境配置以及第一个MapReduce程序-WordCount | Blog</title>
    <meta name="viewport" content="width=device-width, initial-scale=1,user-scalable=no">
    
    <meta name="author" content="yrb">
    

    
    <meta name="description" content="1. 实验前准备工作 1.1 安装hadoop伪分布式系统 1.下载hadoop源文件，hadoop-2.7.1.tar.gz，将其解压在一个文件夹中，这里我解压到了我的电脑的主目录下面，配置参数，主要修改一下几个文件： core-site.xml 12345678910&lt;configuration&gt;  	&lt;property&gt;        	&lt;name&gt;fs.">
<meta property="og:type" content="article">
<meta property="og:title" content="Ubuntu14.04上hadoop2.7.1环境配置以及第一个MapReduce程序-WordCount">
<meta property="og:url" content="https://yanruibo.github.io/2016/05/11/Ubuntu14-04%E4%B8%8Ahadoop2-7-1%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E4%BB%A5%E5%8F%8A%E7%AC%AC%E4%B8%80%E4%B8%AAMapReduce%E7%A8%8B%E5%BA%8F-WordCount/index.html">
<meta property="og:site_name" content="Blog">
<meta property="og:description" content="1. 实验前准备工作 1.1 安装hadoop伪分布式系统 1.下载hadoop源文件，hadoop-2.7.1.tar.gz，将其解压在一个文件夹中，这里我解压到了我的电脑的主目录下面，配置参数，主要修改一下几个文件： core-site.xml 12345678910&lt;configuration&gt;  	&lt;property&gt;        	&lt;name&gt;fs.">
<meta property="og:image" content="https://yanruibo.github.io/2016/05/11/Ubuntu14-04%E4%B8%8Ahadoop2-7-1%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E4%BB%A5%E5%8F%8A%E7%AC%AC%E4%B8%80%E4%B8%AAMapReduce%E7%A8%8B%E5%BA%8F-WordCount/pic/Eclipse-Hadoop-Home-Configure.png">
<meta property="og:image" content="https://yanruibo.github.io/2016/05/11/Ubuntu14-04%E4%B8%8Ahadoop2-7-1%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E4%BB%A5%E5%8F%8A%E7%AC%AC%E4%B8%80%E4%B8%AAMapReduce%E7%A8%8B%E5%BA%8F-WordCount/pic/map-reduce-locations.png">
<meta property="og:image" content="https://yanruibo.github.io/2016/05/11/Ubuntu14-04%E4%B8%8Ahadoop2-7-1%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E4%BB%A5%E5%8F%8A%E7%AC%AC%E4%B8%80%E4%B8%AAMapReduce%E7%A8%8B%E5%BA%8F-WordCount/pic/hadoop-location-conf.png">
<meta property="og:image" content="https://yanruibo.github.io/2016/05/11/Ubuntu14-04%E4%B8%8Ahadoop2-7-1%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E4%BB%A5%E5%8F%8A%E7%AC%AC%E4%B8%80%E4%B8%AAMapReduce%E7%A8%8B%E5%BA%8F-WordCount/pic/all-view.png">
<meta property="article:published_time" content="2016-05-11T12:29:15.000Z">
<meta property="article:modified_time" content="2016-09-17T08:18:00.032Z">
<meta property="article:author" content="yrb">
<meta property="article:tag" content="bigdata system">
<meta property="article:tag" content="hadoop">
<meta property="article:tag" content="spark">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://yanruibo.github.io/2016/05/11/Ubuntu14-04%E4%B8%8Ahadoop2-7-1%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E4%BB%A5%E5%8F%8A%E7%AC%AC%E4%B8%80%E4%B8%AAMapReduce%E7%A8%8B%E5%BA%8F-WordCount/pic/Eclipse-Hadoop-Home-Configure.png">

    
    <link rel="alternative" href="/atom.xml" title="Blog" type="application/atom+xml">
    
    
    <link rel="icon" href="/img/favicon.ico">
    
    
    <link rel="apple-touch-icon" href="/img/jacman.jpg">
    <link rel="apple-touch-icon-precomposed" href="/img/jacman.jpg">
    
    
<link rel="stylesheet" href="/css/style.css">
<link rel="stylesheet" href="/%02.css">
<link rel="stylesheet" href="/.css">

<meta name="generator" content="Hexo 4.2.0"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>

  <body>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><header>
      
<div>
		
			<div id="imglogo">
				<a href="/"><img src="/img/logo.png" alt="Blog" title="Blog"/></a>
			</div>
			
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="Blog">Blog</a></h1>
				<h2 class="blog-motto">Summary for Comprehensive Learning</h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="Menu">
			</a></div>
			<nav class="animated">
				<ul>
					<ul>
					 
						<li><a href="/">Home</a></li>
					
						<li><a href="/archives">Archives</a></li>
					
						<li><a href="/about">About</a></li>
					
					<li>
 					
					<form class="search" action="//google.com/search" method="get" accept-charset="utf-8">
						<label>Search</label>
						<input type="search" id="search" name="q" autocomplete="off" maxlength="20" placeholder="Search" />
						<input type="hidden" name="q" value="site:yanruibo.github.io">
					</form>
					
					</li>
				</ul>
			</nav>			
</div>
    </header>
    <div id="container">
      <div id="main" class="post" itemscope itemprop="blogPost">
  
	<article itemprop="articleBody"> 
		<header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2016/05/11/Ubuntu14-04上hadoop2-7-1环境配置以及第一个MapReduce程序-WordCount/" title="Ubuntu14.04上hadoop2.7.1环境配置以及第一个MapReduce程序-WordCount" itemprop="url">Ubuntu14.04上hadoop2.7.1环境配置以及第一个MapReduce程序-WordCount</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="yrb" target="_blank" itemprop="author">yrb</a>
		
  <p class="article-time">
    <time datetime="2016-05-11T12:29:15.000Z" itemprop="datePublished"> Published 2016-05-11</time>
    
  </p>
</header>
	<div class="article-content">
		
		<div id="toc" class="toc-article">
			<strong class="toc-title">Contents</strong>
		
			<ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#实验前准备工作"><span class="toc-number">1.</span> <span class="toc-text">1. 实验前准备工作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#安装hadoop伪分布式系统"><span class="toc-number">1.1.</span> <span class="toc-text">1.1 安装hadoop伪分布式系统</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#配置eclipse开发环境"><span class="toc-number">1.2.</span> <span class="toc-text">1.2 配置eclipse开发环境</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#实验过程"><span class="toc-number">2.</span> <span class="toc-text">2. 实验过程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#基本要求level-0-1"><span class="toc-number">2.1.</span> <span class="toc-text">2.1 基本要求Level 0 1</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#基本要求level-2"><span class="toc-number">2.2.</span> <span class="toc-text">2.2 基本要求Level 2</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#基本要求level-3"><span class="toc-number">2.3.</span> <span class="toc-text">2.3 基本要求Level 3</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#基本要求level-4"><span class="toc-number">2.4.</span> <span class="toc-text">2.4 基本要求Level 4</span></a></li></ol></li></ol>
		
		</div>
		
		<h2 id="实验前准备工作">1. 实验前准备工作</h2>
<h3 id="安装hadoop伪分布式系统">1.1 安装hadoop伪分布式系统</h3>
<p>1.下载hadoop源文件，hadoop-2.7.1.tar.gz，将其解压在一个文件夹中，这里我解压到了我的电脑的主目录下面，配置参数，主要修改一下几个文件：<br />
core-site.xml<br />
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        	<span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        	<span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://localhost:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> 	<span class="tag">&lt;<span class="name">property</span>&gt;</span>  </span><br><span class="line">        	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </span><br><span class="line">        	<span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/yanruibo/hadoop-2.7.1/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span>  </span><br><span class="line">    	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure> <a id="more"></a> hdfs-site.xml<br />
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        	<span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/yanruibo/hadoop-2.7.1/dfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/yanruibo/hadoop-2.7.1/dfs/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure> mapred-site.xml <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure> yarn-site.xml <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- Site specific YARN configuration properties --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span>	</span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>localhost<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure> 最后在hadoop-env.sh文件的第25行，写入java的path， <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME&#x3D;&#x2F;usr&#x2F;java&#x2F;jdk1.7.0_80</span><br></pre></td></tr></table></figure> 最后为了能在任何路径下都能启动hadoop，需要在path中加入hadoop的路径信息，这里我是为整个系统的用户添加的路径信息，可以在/etc/profile文件中加入如下内容： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_HOME&#x3D;&#x2F;home&#x2F;yanruibo&#x2F;hadoop-2.7.1</span><br><span class="line">export PATH&#x3D;$PATH:$&#123;HADOOP_HOME&#125;&#x2F;bin</span><br></pre></td></tr></table></figure> 配置完这些文件之后，需要安装一些软件。</p>
<p>2.安装ssh</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install ssh</span><br></pre></td></tr></table></figure>
<p>设置ssh免登陆认证 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -P &#39;&#39; -f ~&#x2F;.ssh&#x2F;id_rsa</span><br><span class="line">cat ~&#x2F;.ssh&#x2F;id_rsa.pub &gt;&gt; ~&#x2F;.ssh&#x2F;authorized_keys</span><br></pre></td></tr></table></figure> 执行完这两条命令就可以免登陆了，可以用以下的命令进行测试： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh localhost</span><br></pre></td></tr></table></figure> 会出现下面的结果： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">yanruibo@yanruibo-PC:~$ ssh localhost</span><br><span class="line">The authenticity of host &#39;localhost (127.0.0.1)&#39; can&#39;t be established.</span><br><span class="line">ECDSA key fingerprint is 08:09:86:92:60:c2:62:09:21:3a:89:91:d5:8b:24:de.</span><br><span class="line">Are you sure you want to continue connecting (yes&#x2F;no)? yes</span><br><span class="line">Warning: Permanently added &#39;localhost&#39; (ECDSA) to the list of known hosts.</span><br><span class="line">Welcome to Ubuntu 14.04.3 LTS (GNU&#x2F;Linux 3.19.0-28-generic x86_64)</span><br><span class="line"></span><br><span class="line"> * Documentation:  https:&#x2F;&#x2F;help.ubuntu.com&#x2F;</span><br><span class="line">The programs included with the Ubuntu system are free software;</span><br><span class="line">the exact distribution terms for each program are described in the</span><br><span class="line">individual files in &#x2F;usr&#x2F;share&#x2F;doc&#x2F;*&#x2F;copyright.</span><br><span class="line"></span><br><span class="line">Ubuntu comes with ABSOLUTELY NO WARRANTY, to the extent permitted by</span><br><span class="line">applicable law.</span><br></pre></td></tr></table></figure></p>
<p>3.第一次启动hadoop时要格式化一下hdfs文件系统<br />
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs namenode -format</span><br></pre></td></tr></table></figure> 然后通过在命令行执行，因为我们之前在/etc/profile中设置了hadoop的路径信息，所以在任何路径下执行一下命令都可以 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-all.sh</span><br></pre></td></tr></table></figure> 可以测试一下能否启动成功： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">yanruibo@yanruibo-PC:~$ start-all.sh</span><br><span class="line">This script is![](http:&#x2F;&#x2F;) Deprecated. Instead use start-dfs.sh and start-yarn.sh</span><br><span class="line">Starting namenodes on [localhost]</span><br><span class="line">localhost: starting namenode, logging to &#x2F;home&#x2F;yanruibo&#x2F;hadoop-2.7.1&#x2F;logs&#x2F;hadoop-yanruibo-namenode-yanruibo-PC.out</span><br><span class="line">localhost: starting datanode, logging to &#x2F;home&#x2F;yanruibo&#x2F;hadoop-2.7.1&#x2F;logs&#x2F;hadoop-yanruibo-datanode-yanruibo-PC.out</span><br><span class="line">Starting secondary namenodes [0.0.0.0]</span><br><span class="line">0.0.0.0: starting secondarynamenode, logging to &#x2F;home&#x2F;yanruibo&#x2F;hadoop-2.7.1&#x2F;logs&#x2F;hadoop-yanruibo-secondarynamenode-yanruibo-PC.out</span><br><span class="line">starting yarn daemons</span><br><span class="line">starting resourcemanager, logging to &#x2F;home&#x2F;yanruibo&#x2F;hadoop-2.7.1&#x2F;logs&#x2F;yarn-yanruibo-resourcemanager-yanruibo-PC.out</span><br><span class="line">localhost: starting nodemanager, logging to &#x2F;home&#x2F;yanruibo&#x2F;hadoop-2.7.1&#x2F;logs&#x2F;yarn-yanruibo-nodemanager-yanruibo-PC.out</span><br></pre></td></tr></table></figure> 如果出现以上的信息，说明配置成功。或者执行jps命令 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">yanruibo@yanruibo-PC:~$ jps</span><br><span class="line">11149 SecondaryNameNode</span><br><span class="line">12989 Jps</span><br><span class="line">10941 DataNode</span><br><span class="line">10770 NameNode</span><br><span class="line">11315 ResourceManager</span><br><span class="line">11452 NodeManager</span><br></pre></td></tr></table></figure> 如果出现以上几个进程，也表示执行成功了。<br />
关闭所有进程，相应的是stop-all.sh</p>
<h3 id="配置eclipse开发环境">1.2 配置eclipse开发环境</h3>
<p>首先下载hadoop的eclipse插件，从网上可以下载到，这个插件可以下载网上别人编译好的，也可以用eclipse自己编译，网上有教程，这里为了节省时间，我从网上自己下载了一个eclipse插件，hadoop-eclipse-plugin-2.7.1.jar，也是从csdn上下载的，可以自己搜索下载。下载下来之后放到eclipse安装文件夹下的plugins目录中，重启eclipse即可生效，然后在Window-&gt;Preferences-&gt;Hadoop Map/Reduce项中设置一下hadoop的home。<br />
<img src="pic/Eclipse-Hadoop-Home-Configure.png" align="middle" width="70%" /> 然后切换到eclipse的Map/Reduce视图下面，<br />
<img src="pic/map-reduce-locations.png" align="middle" width="70%" /> 右键 new hadoop location，按如下图所示进行配置，<br />
<img src="pic/hadoop-location-conf.png" align="middle" width="70%" /></p>
<p>注意端口号要配置正确，配置完成这些之后，新建一个Map/Reduce的工程，eclipse就自动把相应依赖的hadoop包加载到工程的path中，你还可以在Project Explorer中的DFS Locations中查看你的hdfs中的文件，能观察到这一点的前提是前面hdfs的端口配置正确，还有就是你执行了start-all.sh，把hadoop的那几个进程都启动起来了。</p>
<p><img src="pic/all-view.png" align="middle" width="70%" /></p>
<p>这里准备工作，即开发环境就搭建好了，下面就开始编写mapreduce程序。</p>
<h2 id="实验过程">2. 实验过程</h2>
<h3 id="基本要求level-0-1">2.1 基本要求Level 0 1</h3>
<p>使用example里面的wordcount算法，统计每个词出现的次数，使用的命令为： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar &#x2F;home&#x2F;yanruibo&#x2F;hadoop-2.7.1&#x2F;share&#x2F;hadoop&#x2F;mapreduce&#x2F;hadoop-mapreduce-examples-2.7.1.jar wordcount &#x2F;tmp&#x2F;bigdata&#x2F;2015&#x2F;english_novel&#x2F;* &#x2F;user&#x2F;2015210978&#x2F;hw1-output1</span><br></pre></td></tr></table></figure> 这里我在eclipse中创建了一个mapreduce工程，包名为：com.alvin.test,有三个类WordCount.java,StringTest.java,Test.java，WordCount.java是实现单词计数的mapreduce程序，其余两个是过滤字符串的标点符号写的测试类。我创建了一个WordCount程序，照着example中的敲过来，然后通过eclipse导出jar包为WordCount1.jar，然后通过如下命令将导出的jar上传到服务器home目录下 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sftp -P 2222 2015210978@thumedia.org</span><br><span class="line">put WordCount1.jar &#x2F;home&#x2F;2015210978</span><br></pre></td></tr></table></figure> 然后ssh登陆到服务器： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -p 2222 2015210978@thumedia.org</span><br></pre></td></tr></table></figure> 在服务器的home目录也就是WordCount1.jar所在的目录，执行jar包，执行的命令为： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_CLASSPATH&#x3D;WordCount1.jar</span><br><span class="line">hadoop com.alvin.test.WordCount &#x2F;tmp&#x2F;bigdata&#x2F;2015&#x2F;english_novel&#x2F;* &#x2F;user&#x2F;2015210978&#x2F;hw1-output1</span><br></pre></td></tr></table></figure> 这里的执行命令参考了《Hadoop The Definitive Guide》。然后就会看到mapreduce任务开始执行了。 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">2015210978@cluster-3-0:~$ hadoop com.alvin.test.WordCount &#x2F;tmp&#x2F;bigdata&#x2F;2015&#x2F;english_novel&#x2F;* &#x2F;user&#x2F;2015210978&#x2F;hw1-output2</span><br><span class="line">15&#x2F;10&#x2F;15 11:07:28 INFO client.RMProxy: Connecting to ResourceManager at cluster29&#x2F;192.168.5.129:8032</span><br><span class="line">15&#x2F;10&#x2F;15 11:07:29 INFO input.FileInputFormat: Total input paths to process : 1464</span><br><span class="line">15&#x2F;10&#x2F;15 11:07:33 INFO mapreduce.JobSubmitter: number of splits:1464</span><br><span class="line">15&#x2F;10&#x2F;15 11:07:34 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1426037502156_0240</span><br><span class="line">15&#x2F;10&#x2F;15 11:07:34 INFO impl.YarnClientImpl: Submitted application application_1426037502156_0240</span><br><span class="line">15&#x2F;10&#x2F;15 11:07:34 INFO mapreduce.Job: The url to track the job: http:&#x2F;&#x2F;cluster29:8088&#x2F;proxy&#x2F;application_1426037502156_0240&#x2F;</span><br><span class="line">15&#x2F;10&#x2F;15 11:07:34 INFO mapreduce.Job: Running job: job_1426037502156_0240</span><br><span class="line">15&#x2F;10&#x2F;15 11:07:49 INFO mapreduce.Job: Job job_1426037502156_0240 running in uber mode : false</span><br><span class="line">15&#x2F;10&#x2F;15 11:09:52 INFO mapreduce.Job: Job job_1426037502156_0240 completed successfully</span><br></pre></td></tr></table></figure> 然后在服务器命令行中查看，并将结果拷贝到服务器的本地文件系统中， <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -ls &#x2F;user&#x2F;2015210978&#x2F;output1</span><br><span class="line">hadoop fs -copyToLocal &#x2F;user&#x2F;2015210978&#x2F;output1 .&#x2F;</span><br></pre></td></tr></table></figure> 最后在sftp中将结果拉到我的电脑中 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">get output1&#x2F;* .&#x2F;</span><br></pre></td></tr></table></figure></p>
<h3 id="基本要求level-2">2.2 基本要求Level 2</h3>
<p>有了单词计数的基本功能，我们查看一下执行结果，发现会有一下的情况： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&quot;&#x2F;de&#x2F;	1</span><br><span class="line">&quot;&#39;Alexander	1</span><br><span class="line">&quot;&#39;Ample.&#39;	1</span><br><span class="line">&quot;3&quot;	1</span><br><span class="line">&quot;52.43!&quot;	1</span><br><span class="line">&quot;&#39;&#39;Twon&#39;t	1</span><br><span class="line">&quot;But--how	2</span><br><span class="line">&quot;Bureaucracy,&quot;--if	1</span><br></pre></td></tr></table></figure> 这样的单词识别效果不是特别好，我们要去掉标点符号和数字以及其他的非英语字符，首先想到的是用正则表达式，于是上网查正则表达式的知识，把最经典的《<a href="http://deerchao.net/tutorials/regex/regex.htm" target="_blank" rel="noopener">正则表达式30分钟入门教程</a>》看完了，同时由搜了一些资料，发现把整个单词中的标点符号全部去掉很容易，但是如果把整个单词的标点符号都去掉的话，上面的第六七八行就会有问题，因为<code>"''Twon't "But--how "Bureaucracy,"--if</code>这三个词两边的标点符号去掉是可以的，但是这三个词中间的标点符号去掉是不合理的，因此我实现的策略是只过滤每个单词两边的标点符号，对于中间的标点符号不进行过滤。具体见下面的代码： <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(Object key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">	StringTokenizer itr = <span class="keyword">new</span> StringTokenizer(value.toString());</span><br><span class="line">	<span class="keyword">while</span> (itr.hasMoreTokens()) &#123;</span><br><span class="line">		<span class="comment">/*</span></span><br><span class="line"><span class="comment">		* 此处进行一些优化 1.去掉单词首尾的空格 2.去掉单词首尾的标点符号和数字</span></span><br><span class="line"><span class="comment">		*/</span></span><br><span class="line">		String s = itr.nextToken();</span><br><span class="line">		<span class="comment">// 去掉单词首尾的空格</span></span><br><span class="line">		s = s.trim();</span><br><span class="line">		<span class="comment">// 去除单词首部的标点符号</span></span><br><span class="line">		s = s.replaceFirst(<span class="string">"(?i)^[^a-zA-Z]+"</span>, <span class="string">""</span>);</span><br><span class="line">		<span class="comment">/*</span></span><br><span class="line"><span class="comment">		* 下面是去除单词尾部的标点符号，先把字符串反转，</span></span><br><span class="line"><span class="comment">		* 然后再调用replaceFirst，</span></span><br><span class="line"><span class="comment">		* 去除反转后的字符串首部的标点符号，再把字符串反转过来</span></span><br><span class="line"><span class="comment">		*/</span></span><br><span class="line">		s = <span class="keyword">new</span> StringBuilder(s).reverse().toString();</span><br><span class="line">		s = s.replaceFirst(<span class="string">"(?i)^[^a-zA-Z]+"</span>, <span class="string">""</span>);</span><br><span class="line">		s = <span class="keyword">new</span> StringBuilder(s).reverse().toString();</span><br><span class="line">		<span class="comment">//过滤之后s的值可能为空，比如全为数字的就变为空</span></span><br><span class="line">		<span class="keyword">if</span> (!s.isEmpty()) &#123;</span><br><span class="line">			word.set(s);</span><br><span class="line">			context.write(word, one);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">s = s.replaceFirst(<span class="string">"(?i)^[^a-zA-Z]+"</span>, <span class="string">""</span>);</span><br></pre></td></tr></table></figure>
<p>解释一下上面的正则表达式，小括号即里面的问好和i表示不去分大小写，紧接着的上角号标示单词的起始位置，中括号即中括号中的内容标示不是大小写字母，所以这句话的意思是不区分大小写把单词首部的不是大小写字母的都去掉。也就是说只要能匹配到哪个正则表达式的字符串都被替换为空字符串，匹配不上的保留。</p>
<h3 id="基本要求level-3">2.3 基本要求Level 3</h3>
<p>将得到的结果合并成一个txt文档，并对每个单词按照字典排序，其实可以通过设置reducer的个数为1，如果结果文件比较小的话，应该就输出到一个文件中了，这一点在实验中进行了验证，以下是设置方法： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;设置reducer的个数 通过设置reducer的个数为1，经过实验验证了输出结果全写在一个文件中了，结果文件也不大。</span><br><span class="line">job.setNumReduceTasks(1);</span><br></pre></td></tr></table></figure> 这样单词就按字典序拍好了，但是大写字母在前小写字母在后，为了不区分大小写，需要用sort命令： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat output2&#x2F;* | sort -t&#39; &#39; -k 1,1df &gt; result.txt</span><br></pre></td></tr></table></figure> sort -t指定分隔符为空格，-k 指定按哪一列进行排序，1,1df标示按第一列进行排序，并且加上-d -f选项，-d按字典序排序，-f忽略大小写。</p>
<h3 id="基本要求level-4">2.4 基本要求Level 4</h3>
<p>要求：自己实现shuffle等函数。</p>
<p>1.查了一些资料，可以自己写combiner函数， 每一个map都可能会产生大量的本地输出，Combiner的作用就是对map端的输出先做一次合并，以减少在map和reduce节点之间的数据传输量，以提高网络IO性能，是MapReduce的一种优化手段之一。Combiner有本地reduce功能（其本质上就是一个<strong>提前的reducer</strong>），在WordCount程序中可以单独写一个Combiner类，也可以用Reducer类来代替，然后在job那儿设置一下： <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">* Combiner类规定继承自Reducer</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyCombiner</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">	<span class="keyword">private</span> IntWritable result = <span class="keyword">new</span> IntWritable();</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span></span></span><br><span class="line"><span class="function">			<span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">		<span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">		<span class="keyword">for</span> (IntWritable val : values) &#123;</span><br><span class="line">			sum += val.get();</span><br><span class="line">		&#125;</span><br><span class="line">		result.set(sum);</span><br><span class="line">		context.write(key, result);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure> 一定不要忘了设置一下 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.setCombinerClass(Combiner.class);</span><br></pre></td></tr></table></figure> 经过实验是可以的。<br />
2.自己写Partitioner <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">* Partitioner 自己写的一个例子，算法也是突发奇想，随便写的，可能分类的效果不好</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyPartitioner</span> <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(Text key,IntWritable value,<span class="keyword">int</span> numReduceTasks)</span> </span>&#123;</span><br><span class="line">		<span class="comment">// TODO Auto-generated method stub</span></span><br><span class="line">		<span class="comment">/*</span></span><br><span class="line"><span class="comment">		* 自己随便想的一个hash函数，把一个单词的每个字符的ASCII码的和乘以一个数再跟reducer的数目取余。</span></span><br><span class="line"><span class="comment">		* 乘以127是为了增加混杂度，取模之后分布更加均匀</span></span><br><span class="line"><span class="comment">		*/</span></span><br><span class="line">		String skey = key.toString();</span><br><span class="line">		<span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">		<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; skey.length(); ++i) &#123;</span><br><span class="line">			sum += skey.charAt(i);</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">return</span> sum * <span class="number">127</span> % numReduceTasks;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure> 同样也不要忘了在设置一下 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//设置Partitioner类</span></span><br><span class="line">job.setPartitionerClass(MyPartitioner.class);</span><br></pre></td></tr></table></figure></p>
  
	</div>
		<footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/data-mining/">data mining</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/bigdata-system/">bigdata system</a><a href="/tags/hadoop/">hadoop</a><a href="/tags/spark/">spark</a>
  </div>

</div>



	<div class="article-share" id="share">
	
	  <div data-url="https://yanruibo.github.io/2016/05/11/Ubuntu14-04%E4%B8%8Ahadoop2-7-1%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E4%BB%A5%E5%8F%8A%E7%AC%AC%E4%B8%80%E4%B8%AAMapReduce%E7%A8%8B%E5%BA%8F-WordCount/" data-title="Ubuntu14.04上hadoop2.7.1环境配置以及第一个MapReduce程序-WordCount | Blog" data-tsina="" class="share clearfix">
	  </div>
	
	</div>


</footer>

   	       
	</article>
	
<nav class="article-nav clearfix">
 
 <div class="prev" >
 <a href="/2016/05/11/Ubuntu配置spark环境和spark-wordcount以及spark常用的actions和transactions/" title="Ubuntu配置spark环境和spark-wordcount以及spark常用的actions和transactions">
  <strong>上一篇：</strong><br/>
  <span>
  Ubuntu配置spark环境和spark-wordcount以及spark常用的actions和transactions</span>
</a>
</div>


<div class="next">
<a href="/2016/05/07/interview-questions-cpp/"  title="interview questions cpp">
 <strong>下一篇：</strong><br/> 
 <span>interview questions cpp
</span>
</a>
</div>

</nav>

	



</div>  
      <div class="openaside"><a class="navbutton" href="#" title="Show Sidebar"></a></div>

  <div id="toc" class="toc-aside">
  <strong class="toc-title">Contents</strong>
 
 <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#实验前准备工作"><span class="toc-number">1.</span> <span class="toc-text">1. 实验前准备工作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#安装hadoop伪分布式系统"><span class="toc-number">1.1.</span> <span class="toc-text">1.1 安装hadoop伪分布式系统</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#配置eclipse开发环境"><span class="toc-number">1.2.</span> <span class="toc-text">1.2 配置eclipse开发环境</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#实验过程"><span class="toc-number">2.</span> <span class="toc-text">2. 实验过程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#基本要求level-0-1"><span class="toc-number">2.1.</span> <span class="toc-text">2.1 基本要求Level 0 1</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#基本要求level-2"><span class="toc-number">2.2.</span> <span class="toc-text">2.2 基本要求Level 2</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#基本要求level-3"><span class="toc-number">2.3.</span> <span class="toc-text">2.3 基本要求Level 3</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#基本要求level-4"><span class="toc-number">2.4.</span> <span class="toc-text">2.4 基本要求Level 4</span></a></li></ol></li></ol>
 
  </div>

<div id="asidepart">
<div class="closeaside"><a class="closebutton" href="#" title="Hide Sidebar"></a></div>
<aside class="clearfix">

  


  
<div class="categorieslist">
	<p class="asidetitle">Categories</p>
		<ul>
		
		  
			<li><a href="/categories/c/" title="c">c<sup>6</sup></a></li>
		  
		
		  
			<li><a href="/categories/cpp/" title="cpp">cpp<sup>4</sup></a></li>
		  
		
		  
			<li><a href="/categories/data-mining/" title="data mining">data mining<sup>2</sup></a></li>
		  
		
		  
			<li><a href="/categories/data-structures-and-algorithms/" title="data structures and algorithms">data structures and algorithms<sup>17</sup></a></li>
		  
		
		  
			<li><a href="/categories/interview/" title="interview">interview<sup>2</sup></a></li>
		  
		
		  
			<li><a href="/categories/linux/" title="linux">linux<sup>2</sup></a></li>
		  
		
		  
			<li><a href="/categories/machine-learning/" title="machine learning">machine learning<sup>7</sup></a></li>
		  
		
		  
			<li><a href="/categories/paper/" title="paper">paper<sup>1</sup></a></li>
		  
		
		</ul>
</div>


  
<div class="tagslist">
	<p class="asidetitle">Tags</p>
		<ul class="clearfix">
		
			
				<li><a href="/tags/machine-learning/" title="machine learning">machine learning<sup>8</sup></a></li>
			
		
			
				<li><a href="/tags/algorithms/" title="algorithms">algorithms<sup>6</sup></a></li>
			
		
			
				<li><a href="/tags/cpp/" title="cpp">cpp<sup>5</sup></a></li>
			
		
			
				<li><a href="/tags/dsa/" title="dsa">dsa<sup>4</sup></a></li>
			
		
			
				<li><a href="/tags/c/" title="c">c<sup>3</sup></a></li>
			
		
			
				<li><a href="/tags/stack/" title="stack">stack<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/queue/" title="queue">queue<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/bigdata-system/" title="bigdata system">bigdata system<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/spark/" title="spark">spark<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/hmm/" title="hmm">hmm<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/gdb/" title="gdb">gdb<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/python/" title="python">python<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/data-structure/" title="data structure">data structure<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/hadoop/" title="hadoop">hadoop<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/paper/" title="paper">paper<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/array/" title="array">array<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/circular-linked-list/" title="circular linked list">circular linked list<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/variable-types/" title="variable types">variable types<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/bayes/" title="bayes">bayes<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/bit-operation/" title="bit operation">bit operation<sup>1</sup></a></li>
			
		
		</ul>
</div>


  <div class="linkslist">
  <p class="asidetitle">Links</p>
    <ul>
        
    </ul>
</div>

  


  <div class="rsspart">
	<a href="/atom.xml" target="_blank" title="rss">RSS</a>
</div>

  <div class="weiboshow">
  <p class="asidetitle">Weibo</p>
    <iframe width="100%" height="119" class="share_self"  frameborder="0" scrolling="no" src="http://widget.weibo.com/weiboshow/index.php?language=&width=0&height=119&fansRow=2&ptype=1&speed=0&skin=9&isTitle=1&noborder=1&isWeibo=0&isFans=0&uid=&verifier=b3593ceb&dpc=1"></iframe>
</div>


</aside>
</div>
    </div>
    <footer><div id="footer" >
	
	<div class="line">
		<span></span>
		<div class="author"></div>
	</div>
	
	
	<section class="info">
		<p> Keep Learning. <br/>
			Summary for fun.</p>
	</section>
	 
	<div class="social-font" class="clearfix">
		
		<a href="http://weibo.com/2176287895" target="_blank" class="icon-weibo" title="微博"></a>
		
		
		
		
		
		
		
		
		
		
	</div>
			
		

		<p class="copyright">
		Powered by <a href="http://hexo.io" target="_blank" title="hexo">hexo</a> and Theme by <a href="https://github.com/wuchong/jacman" target="_blank" title="Jacman">Jacman</a> © 2020 
		
		<a href="/about" target="_blank" title="yrb">yrb</a>
		
		
		</p>
</div>
</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/jquery.qrcode-0.12.0.min.js"></script>

<script type="text/javascript">
$(document).ready(function(){ 
  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      o = $('.openaside');
  c.click(function(){
    a.addClass('fadeOut').css('display', 'none');
    o.css('display', 'block').addClass('fadeIn');
    m.addClass('moveMain');
  });
  o.click(function(){
    o.css('display', 'none').removeClass('beforeFadeIn');
    a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');      
    m.removeClass('moveMain');
  });
  $(window).scroll(function(){
    o.css("top",Math.max(80,260-$(this).scrollTop()));
  });
  
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else{
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
      o.css('display', 'none');
      
      $('#toc.toc-aside').css('display', 'none');
        
    }
  });
});
</script>

<script type="text/javascript">
$(document).ready(function(){ 
  var ai = $('.article-content>iframe'),
      ae = $('.article-content>embed'),
      t  = $('#toc'),
      ta = $('#toc.toc-aside'),
      o  = $('.openaside'),
      c  = $('.closeaside');
  if(ai.length>0){
    ai.wrap('<div class="video-container" />');
  };
  if(ae.length>0){
   ae.wrap('<div class="video-container" />');
  };
  c.click(function(){
    ta.css('display', 'block').addClass('fadeIn');
  });
  o.click(function(){
    ta.css('display', 'none');
  });
  $(window).scroll(function(){
    ta.css("top",Math.max(140,320-$(this).scrollTop()));
  });
});
</script>


<script type="text/javascript">
$(document).ready(function(){ 
  var $this = $('.share'),
      url = $this.attr('data-url'),
      encodedUrl = encodeURIComponent(url),
      title = $this.attr('data-title'),
      tsina = $this.attr('data-tsina'),
      description = $this.attr('description');
  var html = [
  '<div class="hoverqrcode clearfix"></div>',
  '<a class="overlay" id="qrcode"></a>',
  '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="article-share-facebook" target="_blank" title="Facebook"></a>',
  '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="article-share-twitter" target="_blank" title="Twitter"></a>',
  '<a href="#qrcode" class="article-share-qrcode" title="微信"></a>',
  '<a href="http://widget.renren.com/dialog/share?resourceUrl=' + encodedUrl + '&srcUrl=' + encodedUrl + '&title=' + title +'" class="article-share-renren" target="_blank" title="人人"></a>',
  '<a href="http://service.weibo.com/share/share.php?title='+title+'&url='+encodedUrl +'&ralateUid='+ tsina +'&searchPic=true&style=number' +'" class="article-share-weibo" target="_blank" title="微博"></a>',
  '<span title="Share to"></span>'
  ].join('');
  $this.append(html);

  $('.hoverqrcode').hide();

  var myWidth = 0;
  function updatehoverqrcode(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
    var qrsize = myWidth > 1024 ? 200:100;
    var options = {render: 'image', size: qrsize, fill: '#2ca6cb', text: url, radius: 0.5, quiet: 1};
    var p = $('.article-share-qrcode').position();
    $('.hoverqrcode').empty().css('width', qrsize).css('height', qrsize)
                          .css('left', p.left-qrsize/2+20).css('top', p.top-qrsize-10)
                          .qrcode(options);
  };
  $(window).resize(function(){
    $('.hoverqrcode').hide();
  });
  $('.article-share-qrcode').click(function(){
    updatehoverqrcode();
    $('.hoverqrcode').toggle();
  });
  $('.article-share-qrcode').hover(function(){}, function(){
      $('.hoverqrcode').hide();
  });
});   
</script>











<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  $('.article-content').each(function(i){
    $(this).find('img').each(function(){
      if ($(this).parent().hasClass('fancybox')) return;
      var alt = this.alt;
      if (alt) $(this).after('<span class="caption">' + alt + '</span>');
      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox"></a>');
    });
    $(this).find('.fancybox').each(function(){
      $(this).attr('rel', 'article' + i);
    });
  });
  if($.fancybox){
    $('.fancybox').fancybox();
  }
}); 
</script>



<!-- Analytics Begin -->



<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?e6d1f421bbc9962127a50488f9ed37d1";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>



<!-- Analytics End -->

<!-- Totop Begin -->

	<div id="totop">
	<a title="Back to Top"><img src="/img/scrollup.png"/></a>
	</div>
	<script src="/js/totop.js"></script>

<!-- Totop End -->

<!-- MathJax Begin -->
<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<!-- MathJax End -->

<!-- Tiny_search Begin -->

<!-- Tiny_search End --><!-- hexo-inject:begin --><!-- hexo-inject:end -->

  </body>
</html>
